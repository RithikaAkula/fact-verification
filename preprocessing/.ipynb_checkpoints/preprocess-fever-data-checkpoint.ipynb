{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ff7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import cudf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7de1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'evidence_sentence_id', raw_text of the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c26522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54cb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Clean text\n",
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'[\\d]', '', text)\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', ' ', text)\n",
    "\n",
    "\n",
    "def remove_underline(text):\n",
    "    return text.replace('_', ' ')\n",
    "\n",
    "\n",
    "def remove_hyphen(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.lstrip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def decode_special_chars(text):\n",
    "    return re.sub(r'-[A-Z]+-', ' ', text)\n",
    "\n",
    "def remove_newline(text):\n",
    "    return re.sub('\\n', ' ', text)\n",
    "\n",
    "def remove_tabs(text):\n",
    "    return re.sub('\\t', '', text)\n",
    "\n",
    "def remove_intext_tabs(text):\n",
    "    return re.sub(r'(?<!\\d)\\t', ' ', text)\n",
    "\n",
    "def remove_special_tokens(text):\n",
    "    return re.sub(r'-[A-Z]+-', '', text)\n",
    "\n",
    "def remove_quotes(text):\n",
    "    text = re.sub(r'(``|\\' \\')', '', text)\n",
    "    return re.sub(r\"''\", '', text)\n",
    "\n",
    "\n",
    "def clean_text(df: pd.DataFrame, column: str):\n",
    "    \n",
    "    df[column] = df[column].apply(remove_punctuation)\n",
    "    df[column] = df[column].apply(remove_non_ascii)\n",
    "    df[column] = df[column].apply(remove_special_tokens)\n",
    "    df[column] = df[column].apply(remove_extra_space)\n",
    "    df[column] = df[column].apply(remove_quotes)\n",
    "    df[column] = df[column].apply(to_lowercase)\n",
    "    df[column] = df[column].apply(remove_stopwords)\n",
    "    df[column] = df[column].apply(remove_tabs)\n",
    "    df[column] = df[column].apply(remove_extra_space)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1761611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_claim_df():\n",
    "    \n",
    "    cache_dir = '/home/rahvk/data/tmp/cache/fever3' # change this to your own path\n",
    "    claim_dataset = load_dataset('fever', 'v1.0', cache_dir=cache_dir)\n",
    "    claim_df = pd.DataFrame()\n",
    "\n",
    "    for split in ['train']:\n",
    "        # Load train file\n",
    "        claim_split = claim_dataset[split]\n",
    "        claim_d = pd.DataFrame(claim_split)\n",
    "\n",
    "        claim_d = claim_d.drop(columns=[ 'id'])\n",
    "        \n",
    "        # Remove rows with label NOT ENOUGH INFO\n",
    "#         claim_d = claim_d[claim_d['evidence_sentence_id'] != -1]\n",
    "\n",
    "        # Clean claim DataFrame\n",
    "        claim_d['raw_text'] = claim_d['claim']\n",
    "        claim_d = clean_text(df=claim_d, column=\"claim\")\n",
    "        claim_d.rename(columns={'evidence_wiki_url': 'title', 'claim': 'clean_text'}, inplace=True)\n",
    "        claim_df = pd.concat([claim_df, claim_d], axis=0).drop_duplicates()\n",
    "\n",
    "        del claim_split\n",
    "        del claim_d\n",
    "\n",
    "    del claim_dataset\n",
    "    \n",
    "    return claim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4aa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fever (/home/rahvk/data/tmp/cache/fever3/fever/v1.0/1.0.0/7f8936e0558704771b08c7ce9cc202071b29a0050603374507ba61d23c00a58e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6acfd7f41f4f35913f263995dd1098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "claim_df = get_claim_df()\n",
    "claim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP ASIDE NOT ENOUGH INFO ROWS FOR LATER USE\n",
    "neo_rows = claim_df[claim_df['label'] == 'NOT ENOUGH INFO']\n",
    "\n",
    "claim_df = claim_df.drop(noe_rows.index)\n",
    "claim_df = claim_df[claim_df['evidence_sentence_id'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f89b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_df = claim_df.reset_index(drop=True)\n",
    "claim_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lookup_doc_ids(fever_df, parquet_files):\n",
    "    \"\"\"\n",
    "    Lookup and map doc_ids from parquet files to titles in fever_df DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - fever_df (pd.DataFrame): DataFrame containing titles.\n",
    "    - parquet_files (list): List of paths to parquet files.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated fever_df DataFrame with 'doc_id' column added.\n",
    "    \"\"\"\n",
    "    # Load the titles from fever_df into a set for faster lookup\n",
    "    fever_titles = set(fever_df['title'])\n",
    "\n",
    "    # Create an empty dictionary to store doc_id mappings\n",
    "    doc_id_mapping = {}\n",
    "    \n",
    "#     print(\"Looking up.\")\n",
    "    \n",
    "    x = 0\n",
    "    # Iterate over each parquet file\n",
    "    for parquet_file in parquet_files:\n",
    "        \n",
    "        print(f\"Looking up in {x}.parquet\")\n",
    "        # Load the parquet file into a DataFrame\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "\n",
    "        # Filter the DataFrame to include only rows with titles in fever_titles\n",
    "        filtered_df = df[df['title'].isin(fever_titles)]\n",
    "\n",
    "        # Iterate over each row in the filtered DataFrame\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            # Store the doc_id in the doc_id_mapping dictionary\n",
    "            doc_id_mapping[row['title']] = str(row['doc_id'])\n",
    "        \n",
    "        x+=1\n",
    "        \n",
    "    # Map doc_ids to titles in fever_df using the doc_id_mapping dictionary\n",
    "    fever_df['doc_id'] = fever_df['title'].map(doc_id_mapping)\n",
    "\n",
    "    return fever_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31503f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = \"wiki_docs_parquets\"  # Directory containing processed Parquet files\n",
    "processed_files = [read_dir+\"/\"+name for name in sorted(os.listdir(read_dir))]\n",
    "print(processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35788bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_fever_df = lookup_doc_ids(claim_df, processed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52465ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = updated_fever_df.dropna(subset=['doc_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joint_id'] = df['doc_id'].astype(str) + '_' + df['evidence_sentence_id'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by 'raw_text' column and aggregate 'doc_id' values into a list\n",
    "grouped = df.groupby('raw_text')['joint_id'].apply(list).reset_index()\n",
    "\n",
    "# Merge the grouped DataFrame with the original DataFrame on the 'claim' column\n",
    "df = df.merge(grouped, on='raw_text', how='left')\n",
    "\n",
    "# Rename the column containing the lists of 'doc_id' values\n",
    "df.rename(columns={'joint_id_x': 'joint_id', 'joint_id_y': 'joint_ids'}, inplace=True)\n",
    "\n",
    "# Drop duplicate rows based on 'raw_text' column\n",
    "df = df.drop_duplicates(subset='raw_text')\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90afb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['raw_text']=='Jeff Goldblum starred in a film.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9830570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['evidence_annotation_id', 'evidence_id', 'evidence_sentence_id', 'joint_id', 'doc_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c871fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_rows = neo_rows.drop(columns=['evidence_annotation_id', 'evidence_id', 'evidence_sentence_id'])\n",
    "neo_rows_random_sample = neo_rows.sample(n=334)\n",
    "neo_rows_random_sample['joint_ids'] = [[]] * len(neo_rows_random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "supports_rows = df[df['label'] == 'SUPPORTS']\n",
    "supports_rows_random_sample = supports_rows.sample(n=333)\n",
    "\n",
    "refutes_rows = df[df['label'] == 'REFUTES']\n",
    "refutes_rows_random_sample = refutes_rows.sample(n=333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([supports_rows_random_sample, refutes_rows_random_sample, neo_rows_random_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54655ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKS\n",
    "def perform_checks(df):\n",
    "    \n",
    "    print(\"Number of NOT ENOUGH INFO rows: \",(df['label'] == 'NOT ENOUGH INFO').sum())\n",
    "    print(\"Number of proper SUPPORTS rows: \",((df['label'] == 'SUPPORTS').sum()))\n",
    "    print(\"Number of proper REFUTES rows: \",((df['label'] == 'REFUTES').sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1630d3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NOT ENOUGH INFO rows:  0\n",
      "Number of proper SUPPORTS rows:  36473\n",
      "Number of proper REFUTES rows:  12503\n"
     ]
    }
   ],
   "source": [
    "perform_checks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6d4e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('processed_fever/fever-train-1000.parquet')\n",
    "df.to_json('processed_fever/fever-train-1000.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f708d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python faiss_1.7.4",
   "language": "python",
   "name": "faiss_1.7.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
