{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install simpletransformers\n",
    "! pip install tensorboardX\n",
    "! pip install Unidecode\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import cudf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_paraquet_files():\n",
    "    \n",
    "    ids = [str(i) for i in range(10)]\n",
    "    base_url = \"https://huggingface.co/api/datasets/fever/parquet/wiki_pages/wikipedia_pages/\"\n",
    "    cach_dir = '/home/rahvk/data/tmp/cache' # change this to your own path\n",
    "    output_dir = 'wiki_pages_parquets'\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "    for index in ids:\n",
    "        data_files = {\"wikipedia_pages\": base_url + f\"{index}.parquet\"}\n",
    "        wiki = load_dataset(\"parquet\", data_files=data_files, split=\"wikipedia_pages\", cache_dir=cache_dir)\n",
    "        \n",
    "        wiki.to_csv(f\"{output_dir}/{index}_parquet_wiki.csv\")\n",
    "        \n",
    "        del wiki\n",
    "        \n",
    "        print(f\"completed downloading {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_wiki_paraquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Clean text\n",
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'[\\d]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', ' ', text)\n",
    "\n",
    "def remove_underline(text):\n",
    "    return text.replace('_', ' ')\n",
    "\n",
    "def remove_hyphen(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.lstrip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def decode_special_chars(text):\n",
    "    return re.sub(r'-[A-Z]+-', ' ', text)\n",
    "\n",
    "def remove_newline(text):\n",
    "    return re.sub('\\n', ' ', text)\n",
    "\n",
    "def remove_tabs(text):\n",
    "    return re.sub('\\t', '', text)\n",
    "\n",
    "def remove_intext_tabs(text):\n",
    "    return re.sub(r'(?<!\\d)\\t', ' ', text)\n",
    "\n",
    "def split_wiki_lines(lines):\n",
    "    \"\"\"\n",
    "    Seperates lines in Wiki pages based on line index followed by \n",
    "    new tab char.\n",
    "    @param lines - lines column from wikipedia pages DataFrame.\n",
    "    ______\n",
    "    Returns pd.DataFrame: new column containing list of lines \n",
    "            in wikipedia pages separated by comma.\n",
    "    \"\"\"\n",
    "    lines = re.split(r'\\d+\\t', lines)\n",
    "    lines = lines[1:len(lines)-1]\n",
    "    return lines\n",
    "\n",
    "def remove_special_tokens(text):\n",
    "    return re.sub(r'-[A-Z]+-', '', text)\n",
    "\n",
    "def remove_quotes(text):\n",
    "    text = re.sub(r'(``|\\' \\')', '', text)\n",
    "    return re.sub(r\"''\", '', text)\n",
    "\n",
    "def remove_empty_lines(lines):\n",
    "    return [s for s in lines if s != '\\n']\n",
    "\n",
    "def lines_to_dict(lines):\n",
    "    lines_dict = {}\n",
    "    for line in lines.split('\\n'):\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            parts = line.split('\\t')\n",
    "            index = parts[0]\n",
    "            value = '\\t'.join(parts[1:])  # Reconstruct the value part\n",
    "            lines_dict[index] = value\n",
    "    # Remove empty strings from values\n",
    "    lines_dict = {k: v for k, v in lines_dict.items() if v}\n",
    "    return lines_dict\n",
    "\n",
    "\n",
    "def clean_text(df: pd.DataFrame, column: str):\n",
    "    \n",
    "#     df[column] = df[column].apply(remove_special_tokens)\n",
    "#     df[column] = df[column].apply(remove_extra_space)\n",
    "    df[column] = df[column].apply(lines_to_dict)\n",
    "#     df[column] = df[column].apply(remove_empty_lines)\n",
    "\n",
    "    return df \n",
    "\n",
    "def clean_lines(df: pd.DataFrame, column: str):\n",
    "\n",
    "    df[column] = df[column].apply(remove_special_tokens)\n",
    "    df[column] = df[column].apply(remove_punctuation)\n",
    "    df[column] = df[column].apply(remove_non_ascii)\n",
    "    df[column] = df[column].apply(to_lowercase)\n",
    "    df[column] = df[column].apply(remove_stopwords)\n",
    "    df[column] = df[column].apply(remove_tabs)\n",
    "    df[column] = df[column].apply(remove_extra_space)\n",
    "    df[column] = df[column].apply(remove_quotes)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_df(df):\n",
    "    \n",
    "    print(\"creating dictionary out of lines column\")\n",
    "    df['lines'] = df['lines'].apply(lines_to_dict)\n",
    "    \n",
    "    # Initialize an empty list to store expanded rows\n",
    "    expanded_rows = []\n",
    "    \n",
    "    print(\"splitting lines into separate rows\")\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Iterate over each key-value pair in the 'lines' dictionary\n",
    "        for key, value in row['lines'].items():\n",
    "            # Create a new row with the key-value pair and other columns from the original DataFrame\n",
    "            new_row = row.drop('lines').to_dict()\n",
    "            new_row.update({'passage_id': key, 'passage_content': value})\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame from the expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    \n",
    "    del df, expanded_rows\n",
    "\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(file_index, uid_start):\n",
    "    print(f\"Started processing file - {file_index}\")\n",
    "    wiki_csv = pd.read_csv(f\"wiki_pages_parquets/{file_index}_parquet_wiki.csv\")\n",
    "\n",
    "    # Remove \"text\" column\n",
    "    df_v0 = wiki_csv.drop(columns=['text'])\n",
    "    del wiki_csv\n",
    "    # Remove NaN rows\n",
    "    df_v0 = df_v0.dropna()\n",
    "    \n",
    "    # Adjust index to create a unique identifier\n",
    "    df_v0.reset_index(drop=True, inplace=True)\n",
    "    df_v0.index += uid_start\n",
    "    # Store as doc_id\n",
    "    df_v0['doc_id'] = list(df_v0.index)\n",
    "    next_uid_start = df_v0.index[-1] + 1\n",
    "\n",
    "    # split the lines column into separate rows\n",
    "    df_v1 = explode_df(df_v0)\n",
    "    del df_v0\n",
    "    \n",
    "    print(\"Further cleaning lines\")\n",
    "    df_v1['raw_passage_content'] = df_v1['passage_content']\n",
    "    df_v2 = clean_lines(df=df_v1, column='passage_content')\n",
    "    del df_v1\n",
    "    \n",
    "    df_v2.rename(columns={'id': 'title', 'passage_content': 'clean_passage_content'}, inplace=True)\n",
    "    \n",
    "    # Return processed DataFrame and last UID\n",
    "    return df_v2, next_uid_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_store_parquet_files_with_doc_id():\n",
    "\n",
    "    ids = [str(i) for i in range(10)]  # 10 files\n",
    "    output_dir = \"wiki_passages_parquets_2\"  # Directory to store processed Parquet files\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "    global uid_start\n",
    "    uid_start = 0\n",
    "    \n",
    "    # Process each file and store the processed DataFrame as a separate Parquet file\n",
    "    for index in ids:\n",
    "\n",
    "        df_processed, uid_start = process_single_file(index, uid_start)\n",
    "        df_processed['joint_id'] = df_processed['doc_id'].astype(str) + '_' + df_processed['passage_id'].astype(str)\n",
    "        \n",
    "        # Store the df\n",
    "        output_filename = os.path.join(output_dir, f\"{index}.parquet\")\n",
    "        df_processed.to_parquet(output_filename, index=False)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Finish\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing file - 0\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 1\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 2\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 3\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 4\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 5\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 6\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 7\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 8\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 9\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_and_store_parquet_files_with_doc_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_files_to_json():\n",
    "    read_dir = \"wiki_passages_parquets_2\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"wiki_passages_jsons_2\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "\n",
    "        # Convert unique_id to string\n",
    "        wiki_df['passage_id'] = wiki_df['passage_id'].astype(str)\n",
    "        wiki_df['doc_id'] = wiki_df['doc_id'].astype(str)\n",
    "        wiki_df['joint_id'] = wiki_df['joint_id'].astype(str)\n",
    "\n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.parquet and saved as 0.json\n",
      "Processed 1.parquet and saved as 1.json\n",
      "Processed 2.parquet and saved as 2.json\n",
      "Processed 3.parquet and saved as 3.json\n",
      "Processed 4.parquet and saved as 4.json\n",
      "Processed 5.parquet and saved as 5.json\n",
      "Processed 6.parquet and saved as 6.json\n",
      "Processed 7.parquet and saved as 7.json\n",
      "Processed 8.parquet and saved as 8.json\n",
      "Processed 9.parquet and saved as 9.json\n",
      "Conversion completed\n"
     ]
    }
   ],
   "source": [
    "process_parquet_files_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_pyserini_format():\n",
    "    \n",
    "    '''\n",
    "    Required format:\n",
    "    {\n",
    "      \"id\": \"doc1\",\n",
    "      \"contents\": \"this is the contents.\"\n",
    "    }\n",
    "    '''\n",
    "    read_dir = \"wiki_docs_parquets\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"pyserini_format_docs_clean_texts\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "        \n",
    "        wiki_df.drop(['title', 'doc_id'], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        wiki_df = wiki_df.rename(columns={'clean_passage_content': 'contents', 'joint_id': 'id'})\n",
    "        \n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['passage_id', 'raw_passage_content'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_json_to_pyserini_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mconvert_json_to_pyserini_format\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m processed_files:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Read the Parquet file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     wiki_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(read_dir, filename))\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mwiki_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpassage_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoc_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_passage_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Rename columns\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     wiki_df \u001b[38;5;241m=\u001b[39m wiki_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_passage_content\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoint_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m~/data/tmp/anaconda3/envs/faiss_1.7.4/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/tmp/anaconda3/envs/faiss_1.7.4/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/tmp/anaconda3/envs/faiss_1.7.4/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/tmp/anaconda3/envs/faiss_1.7.4/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/data/tmp/anaconda3/envs/faiss_1.7.4/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/data/tmp/anaconda3/envs/faiss_1.7.4/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['passage_id', 'raw_passage_content'] not found in axis\""
     ]
    }
   ],
   "source": [
    "convert_json_to_pyserini_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_pyserini_format_raw_texts():\n",
    "    \n",
    "    '''\n",
    "    Required format:\n",
    "    {\n",
    "      \"id\": \"doc1\",\n",
    "      \"contents\": \"this is the contents.\"\n",
    "    }\n",
    "    '''\n",
    "    read_dir = \"wiki_passages_parquets_2\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"pyserini_format_passages_raw_texts\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "\n",
    "        wiki_df.drop(['passage_id', 'title', 'doc_id', 'clean_passage_content'], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        wiki_df = wiki_df.rename(columns={'raw_passage_content': 'contents', 'joint_id': 'id'})\n",
    "        \n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.parquet and saved as 0.json\n",
      "Processed 1.parquet and saved as 1.json\n",
      "Processed 2.parquet and saved as 2.json\n",
      "Processed 3.parquet and saved as 3.json\n",
      "Processed 4.parquet and saved as 4.json\n",
      "Processed 5.parquet and saved as 5.json\n",
      "Processed 6.parquet and saved as 6.json\n",
      "Processed 7.parquet and saved as 7.json\n",
      "Processed 8.parquet and saved as 8.json\n",
      "Processed 9.parquet and saved as 9.json\n",
      "Conversion completed\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_pyserini_format_raw_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python faiss_1.7.4",
   "language": "python",
   "name": "faiss_1.7.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
