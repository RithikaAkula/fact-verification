{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install simpletransformers\n",
    "! pip install tensorboardX\n",
    "! pip install Unidecode\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import cudf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_paraquet_files():\n",
    "    \n",
    "    ids = [str(i) for i in range(10)]\n",
    "    base_url = \"https://huggingface.co/api/datasets/fever/parquet/wiki_pages/wikipedia_pages/\"\n",
    "    cach_dir = '/home/rahvk/data/tmp/cache' # change this to your own path\n",
    "    output_dir = 'wiki_pages_parquets'\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "    for index in ids:\n",
    "        data_files = {\"wikipedia_pages\": base_url + f\"{index}.parquet\"}\n",
    "        wiki = load_dataset(\"parquet\", data_files=data_files, split=\"wikipedia_pages\", cache_dir=cache_dir)\n",
    "        \n",
    "        wiki.to_csv(f\"{output_dir}/{index}_parquet_wiki.csv\")\n",
    "        \n",
    "        del wiki\n",
    "        \n",
    "        print(f\"completed downloading {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_wiki_paraquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Clean text\n",
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'[\\d]', '', text)\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', ' ', text)\n",
    "\n",
    "\n",
    "def remove_underline(text):\n",
    "    return text.replace('_', ' ')\n",
    "\n",
    "\n",
    "def remove_hyphen(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.lstrip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def decode_special_chars(text):\n",
    "    return re.sub(r'-[A-Z]+-', ' ', text)\n",
    "\n",
    "def remove_newline(text):\n",
    "    return re.sub('\\n', ' ', text)\n",
    "\n",
    "def remove_tabs(text):\n",
    "    return re.sub('\\t', '', text)\n",
    "\n",
    "def remove_intext_tabs(text):\n",
    "    return re.sub(r'(?<!\\d)\\t', ' ', text)\n",
    "\n",
    "def split_wiki_lines(lines):\n",
    "    \"\"\"\n",
    "    Seperates lines in Wiki pages based on line index followed by \n",
    "    new tab char.\n",
    "    @param lines - lines column from wikipedia pages DataFrame.\n",
    "    ______\n",
    "    Returns pd.DataFrame: new column containing list of lines \n",
    "            in wikipedia pages separated by comma.\n",
    "    \"\"\"\n",
    "    lines = re.split(r'\\d+\\t', lines)\n",
    "    lines = lines[1:len(lines)-1]\n",
    "    return lines\n",
    "\n",
    "def remove_special_tokens(text):\n",
    "    return re.sub(r'-[A-Z]+-', '', text)\n",
    "\n",
    "def remove_quotes(text):\n",
    "    text = re.sub(r'(``|\\' \\')', '', text)\n",
    "    return re.sub(r\"''\", '', text)\n",
    "\n",
    "def remove_empty_lines(lines):\n",
    "    return [s for s in lines if s != '\\n']\n",
    "\n",
    "\n",
    "def clean_text(df: pd.DataFrame, column: str):\n",
    "    \n",
    "    df[column] = df[column].apply(remove_special_tokens)\n",
    "    df[column] = df[column].apply(remove_extra_space)\n",
    "    df[column] = df[column].apply(remove_quotes)\n",
    "    df[column] = df[column].apply(split_wiki_lines)\n",
    "    df[column] = df[column].apply(remove_empty_lines)\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(file_index, uid_start):\n",
    "    print(f\"Started processing file - {file_index}\")\n",
    "    wiki_csv = pd.read_csv(f\"wiki_pages_parquets/{file_index}_parquet_wiki.csv\")\n",
    "\n",
    "    # Remove \"lines\" column\n",
    "    df_v0 = wiki_csv.drop(columns=['text'])\n",
    "    del wiki_csv\n",
    "    # Remove NaN rows\n",
    "    df_v0 = df_v0.dropna()\n",
    "\n",
    "    # Clean Text\n",
    "    df_v1 = clean_text(df=df_v0, column='lines')\n",
    "    del df_v0\n",
    "    # Drop rows where `id` is NaN (or empty)\n",
    "    df_v1['id'].replace('', np.nan, inplace=True)\n",
    "    df_v2 = df_v1[df_v1['id'].notna()]\n",
    "    df_v2.rename(columns={'id': 'title'}, inplace=True)\n",
    "\n",
    "    del df_v1\n",
    "\n",
    "    # Adjust index to create a unique identifier\n",
    "    df_v2.reset_index(drop=True, inplace=True)\n",
    "    df_v2.index += uid_start\n",
    "\n",
    "    # Convert to cudf\n",
    "    df_v2_gpu = cudf.DataFrame.from_pandas(df_v2)\n",
    "\n",
    "    # Return processed DataFrame and last UID\n",
    "    return df_v2_gpu, df_v2.index[-1] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global counter for generating unique IDs\n",
    "global_unique_id = 0\n",
    "\n",
    "def process_and_store_parquet_files():\n",
    "    global global_unique_id  # Ensure we are using the global variable\n",
    "    ids = [str(i) for i in range(10)]  # 10 files\n",
    "    output_dir = \"processed_wiki\"  # Directory to store processed Parquet files\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "    \n",
    "    # Process each file and store the processed DataFrame as a separate Parquet file\n",
    "    for index in ids:\n",
    "        uid_start = global_unique_id  # Start from the current global_unique_id\n",
    "        df_processed, _ = process_single_file(index, uid_start)\n",
    "        \n",
    "        exploded_df = df_processed.explode('lines').reset_index(drop=True)\n",
    "        del df_processed\n",
    "        \n",
    "        # Generate unique IDs for each row within the Parquet file\n",
    "        exploded_df['unique_id'] = cudf.Series(range(uid_start, uid_start + len(exploded_df)))\n",
    "        \n",
    "        output_filename = os.path.join(output_dir, f\"{index}.parquet\")\n",
    "        exploded_df.to_parquet(output_filename, index=False)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Update global_unique_id for the next file\n",
    "        global_unique_id += len(exploded_df)\n",
    "        del exploded_df\n",
    "\n",
    "    print(\"Processing and storing Parquet files complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing file - 0\n",
      "Started processing file - 1\n",
      "Started processing file - 2\n",
      "Started processing file - 3\n",
      "Started processing file - 4\n",
      "Started processing file - 5\n",
      "Started processing file - 6\n",
      "Started processing file - 7\n",
      "Started processing file - 8\n",
      "Started processing file - 9\n",
      "Processing and storing Parquet files complete\n"
     ]
    }
   ],
   "source": [
    "process_and_store_parquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_files_to_json():\n",
    "    read_dir = \"processed_wiki\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"processed_wiki_jsons\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "\n",
    "        # Convert unique_id to string\n",
    "        wiki_df['unique_id'] = wiki_df['unique_id'].astype(str)\n",
    "\n",
    "        # Rename columns\n",
    "        wiki_df = wiki_df.rename(columns={'lines': 'contents', 'unique_id': 'id'})\n",
    "\n",
    "        # Drop the 'title' column\n",
    "        wiki_df = wiki_df.drop(columns=['title'])\n",
    "\n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.parquet and saved as 0.json\n",
      "Processed 1.parquet and saved as 1.json\n",
      "Processed 2.parquet and saved as 2.json\n",
      "Processed 3.parquet and saved as 3.json\n",
      "Processed 4.parquet and saved as 4.json\n",
      "Processed 5.parquet and saved as 5.json\n",
      "Processed 6.parquet and saved as 6.json\n",
      "Processed 7.parquet and saved as 7.json\n",
      "Processed 8.parquet and saved as 8.json\n",
      "Processed 9.parquet and saved as 9.json\n",
      "Conversion completed\n"
     ]
    }
   ],
   "source": [
    "process_parquet_files_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING DUE TO CUDF MEMORY CONSTRAINTS\n",
    "\n",
    "def concatenate_processed_parquet_files():\n",
    "    output_dir = \"processed_data\"  # Directory containing processed Parquet files\n",
    "    processed_files = sorted(os.listdir(output_dir))\n",
    "    batch_size = 2  # Define your batch size based on memory constraints\n",
    "    all_batches = []\n",
    "\n",
    "    for i in range(0, len(processed_files), batch_size):\n",
    "        batch_files = processed_files[i:i + batch_size]\n",
    "        batch_dfs = [cudf.read_parquet(os.path.join(output_dir, filename)) for filename in batch_files]\n",
    "        \n",
    "        # Concatenate smaller batches\n",
    "        concatenated_batch = cudf.DataFrame()\n",
    "        for df in batch_dfs:\n",
    "            concatenated_batch = cudf.concat([concatenated_batch, df], axis=0)\n",
    "        all_batches.append(concatenated_batch)\n",
    "\n",
    "        del batch_dfs, concatenated_batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Reading and concatenating processed Parquet files complete\")\n",
    "\n",
    "    # Concatenate all batches into a final DataFrame\n",
    "    final_df = cudf.concat(all_batches, axis=0)\n",
    "\n",
    "    # Remove processed Parquet files\n",
    "    for filename in processed_files:\n",
    "        os.remove(os.path.join(output_dir, filename))\n",
    "    os.rmdir(output_dir)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python faiss_1.7.4",
   "language": "python",
   "name": "faiss_1.7.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
