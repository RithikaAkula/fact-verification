{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install simpletransformers\n",
    "! pip install tensorboardX\n",
    "! pip install Unidecode\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import cudf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_paraquet_files():\n",
    "    \n",
    "    ids = [str(i) for i in range(10)]\n",
    "    base_url = \"https://huggingface.co/api/datasets/fever/parquet/wiki_pages/wikipedia_pages/\"\n",
    "    cach_dir = '/home/rahvk/data/tmp/cache' # change this to your own path\n",
    "    output_dir = 'wiki_pages_parquets'\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "    for index in ids:\n",
    "        data_files = {\"wikipedia_pages\": base_url + f\"{index}.parquet\"}\n",
    "        wiki = load_dataset(\"parquet\", data_files=data_files, split=\"wikipedia_pages\", cache_dir=cache_dir)\n",
    "        \n",
    "        wiki.to_csv(f\"{output_dir}/{index}_parquet_wiki.csv\")\n",
    "        \n",
    "        del wiki\n",
    "        \n",
    "        print(f\"completed downloading {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_wiki_paraquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Clean text\n",
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'[\\d]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', ' ', text)\n",
    "\n",
    "def remove_underline(text):\n",
    "    return text.replace('_', ' ')\n",
    "\n",
    "def remove_hyphen(text):\n",
    "    return text.replace('-', ' ')\n",
    "\n",
    "def remove_leading_whitespace(text):\n",
    "    return text.lstrip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def decode_special_chars(text):\n",
    "    return re.sub(r'-[A-Z]+-', ' ', text)\n",
    "\n",
    "def remove_newline(text):\n",
    "    return re.sub('\\n', ' ', text)\n",
    "\n",
    "def remove_tabs(text):\n",
    "    return re.sub('\\t', '', text)\n",
    "\n",
    "def remove_intext_tabs(text):\n",
    "    return re.sub(r'(?<!\\d)\\t', ' ', text)\n",
    "\n",
    "def split_wiki_lines(lines):\n",
    "    \"\"\"\n",
    "    Seperates lines in Wiki pages based on line index followed by \n",
    "    new tab char.\n",
    "    @param lines - lines column from wikipedia pages DataFrame.\n",
    "    ______\n",
    "    Returns pd.DataFrame: new column containing list of lines \n",
    "            in wikipedia pages separated by comma.\n",
    "    \"\"\"\n",
    "    lines = re.split(r'\\d+\\t', lines)\n",
    "    lines = lines[1:len(lines)-1]\n",
    "    return lines\n",
    "\n",
    "def remove_special_tokens(text):\n",
    "    return re.sub(r'-[A-Z]+-', '', text)\n",
    "\n",
    "def remove_quotes(text):\n",
    "    text = re.sub(r'(``|\\' \\')', '', text)\n",
    "    return re.sub(r\"''\", '', text)\n",
    "\n",
    "def remove_empty_lines(lines):\n",
    "    return [s for s in lines if s != '\\n']\n",
    "\n",
    "def lines_to_dict(lines):\n",
    "    lines_dict = {}\n",
    "    for line in lines.split('\\n'):\n",
    "        if line.strip():  # Ignore empty lines\n",
    "            parts = line.split('\\t')\n",
    "            index = parts[0]\n",
    "            value = '\\t'.join(parts[1:])  # Reconstruct the value part\n",
    "            lines_dict[index] = value\n",
    "    # Remove empty strings from values\n",
    "    lines_dict = {k: v for k, v in lines_dict.items() if v}\n",
    "    return lines_dict\n",
    "\n",
    "\n",
    "def clean_text(df: pd.DataFrame, column: str):\n",
    "    \n",
    "#     df[column] = df[column].apply(remove_special_tokens)\n",
    "#     df[column] = df[column].apply(remove_extra_space)\n",
    "    df[column] = df[column].apply(lines_to_dict)\n",
    "#     df[column] = df[column].apply(remove_empty_lines)\n",
    "\n",
    "    return df \n",
    "\n",
    "def clean_lines(df: pd.DataFrame, column: str):\n",
    "\n",
    "    df[column] = df[column].apply(remove_special_tokens)\n",
    "    df[column] = df[column].apply(remove_punctuation)\n",
    "    df[column] = df[column].apply(remove_non_ascii)\n",
    "    df[column] = df[column].apply(to_lowercase)\n",
    "    df[column] = df[column].apply(remove_stopwords)\n",
    "    df[column] = df[column].apply(remove_tabs)\n",
    "    df[column] = df[column].apply(remove_extra_space)\n",
    "    df[column] = df[column].apply(remove_quotes)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_df(df):\n",
    "    \n",
    "    print(\"creating dictionary out of lines column\")\n",
    "    df['lines'] = df['lines'].apply(lines_to_dict)\n",
    "    \n",
    "    # Initialize an empty list to store expanded rows\n",
    "    expanded_rows = []\n",
    "    \n",
    "    print(\"splitting lines into separate rows\")\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        # Iterate over each key-value pair in the 'lines' dictionary\n",
    "        for key, value in row['lines'].items():\n",
    "            # Create a new row with the key-value pair and other columns from the original DataFrame\n",
    "            new_row = row.drop('lines').to_dict()\n",
    "            new_row.update({'passage_id': key, 'passage_content': value})\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame from the expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    \n",
    "    del df, expanded_rows\n",
    "\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(file_index, uid_start):\n",
    "    print(f\"Started processing file - {file_index}\")\n",
    "    wiki_csv = pd.read_csv(f\"wiki_pages_parquets/{file_index}_parquet_wiki.csv\")\n",
    "\n",
    "    # Remove \"text\" column\n",
    "    df_v0 = wiki_csv.drop(columns=['text'])\n",
    "    del wiki_csv\n",
    "    # Remove NaN rows\n",
    "    df_v0 = df_v0.dropna()\n",
    "    \n",
    "    # Adjust index to create a unique identifier\n",
    "    df_v0.reset_index(drop=True, inplace=True)\n",
    "    df_v0.index += uid_start\n",
    "    # Store as doc_id\n",
    "    df_v0['doc_id'] = list(df_v0.index)\n",
    "    next_uid_start = df_v0.index[-1] + 1\n",
    "\n",
    "    # split the lines column into separate rows\n",
    "    df_v1 = explode_df(df_v0)\n",
    "    del df_v0\n",
    "    \n",
    "    print(\"Further cleaning lines\")\n",
    "    df_v1['raw_passage_content'] = df_v1['passage_content']\n",
    "    df_v2 = clean_lines(df=df_v1, column='passage_content')\n",
    "    del df_v1\n",
    "    \n",
    "    df_v2.rename(columns={'id': 'title', 'passage_content': 'clean_passage_content'}, inplace=True)\n",
    "    \n",
    "    # Return processed DataFrame and last UID\n",
    "    return df_v2, next_uid_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_store_parquet_files_with_doc_id():\n",
    "\n",
    "    ids = [str(i) for i in range(10)]  # 10 files\n",
    "    output_dir = \"wiki_passages_parquets_2\"  # Directory to store processed Parquet files\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "    global uid_start\n",
    "    uid_start = 0\n",
    "    \n",
    "    # Process each file and store the processed DataFrame as a separate Parquet file\n",
    "    for index in ids:\n",
    "\n",
    "        df_processed, uid_start = process_single_file(index, uid_start)\n",
    "        df_processed['joint_id'] = df_processed['doc_id'].astype(str) + '_' + df_processed['passage_id'].astype(str)\n",
    "        \n",
    "        # Store the df\n",
    "        output_filename = os.path.join(output_dir, f\"{index}.parquet\")\n",
    "        df_processed.to_parquet(output_filename, index=False)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Finish\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing file - 0\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 1\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 2\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 3\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 4\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 5\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 6\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 7\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 8\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n",
      "Started processing file - 9\n",
      "creating dictionary out of lines column\n",
      "splitting lines into separate rows\n",
      "Further cleaning lines\n",
      "Finish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_and_store_parquet_files_with_doc_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_files_to_json():\n",
    "    read_dir = \"wiki_passages_parquets_2\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"wiki_passages_jsons_2\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "\n",
    "        # Convert unique_id to string\n",
    "        wiki_df['passage_id'] = wiki_df['passage_id'].astype(str)\n",
    "        wiki_df['doc_id'] = wiki_df['doc_id'].astype(str)\n",
    "        wiki_df['joint_id'] = wiki_df['joint_id'].astype(str)\n",
    "\n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.parquet and saved as 0.json\n",
      "Processed 1.parquet and saved as 1.json\n",
      "Processed 2.parquet and saved as 2.json\n",
      "Processed 3.parquet and saved as 3.json\n",
      "Processed 4.parquet and saved as 4.json\n",
      "Processed 5.parquet and saved as 5.json\n",
      "Processed 6.parquet and saved as 6.json\n",
      "Processed 7.parquet and saved as 7.json\n",
      "Processed 8.parquet and saved as 8.json\n",
      "Processed 9.parquet and saved as 9.json\n",
      "Conversion completed\n"
     ]
    }
   ],
   "source": [
    "process_parquet_files_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_pyserini_format():\n",
    "    \n",
    "    '''\n",
    "    Required format:\n",
    "    {\n",
    "      \"id\": \"doc1\",\n",
    "      \"contents\": \"this is the contents.\"\n",
    "    }\n",
    "    '''\n",
    "    read_dir = \"wiki_passages_parquets_2\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"pyserini_format_docs_clean_texts\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "        \n",
    "        wiki_df.drop(['passage_id', 'title', 'doc_id', 'raw_passage_content'], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        wiki_df = wiki_df.rename(columns={'clean_passage_content': 'contents', 'joint_id': 'id'})\n",
    "        \n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.parquet and saved as 0.json\n",
      "Processed 1.parquet and saved as 1.json\n",
      "Processed 2.parquet and saved as 2.json\n",
      "Processed 3.parquet and saved as 3.json\n",
      "Processed 4.parquet and saved as 4.json\n",
      "Processed 5.parquet and saved as 5.json\n",
      "Processed 6.parquet and saved as 6.json\n",
      "Processed 7.parquet and saved as 7.json\n",
      "Processed 8.parquet and saved as 8.json\n",
      "Processed 9.parquet and saved as 9.json\n",
      "Conversion completed\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_pyserini_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_pyserini_format_raw_texts():\n",
    "    \n",
    "    '''\n",
    "    Required format:\n",
    "    {\n",
    "      \"id\": \"doc1\",\n",
    "      \"contents\": \"this is the contents.\"\n",
    "    }\n",
    "    '''\n",
    "    read_dir = \"wiki_passages_parquets_2\"  # Directory containing processed Parquet files\n",
    "    output_dir = \"pyserini_format_docs_raw_texts\"\n",
    "    processed_files = sorted(os.listdir(read_dir))\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "\n",
    "    for filename in processed_files:\n",
    "        # Read the Parquet file\n",
    "        wiki_df = pd.read_parquet(os.path.join(read_dir, filename))\n",
    "\n",
    "        wiki_df.drop(['passage_id', 'title', 'doc_id', 'clean_passage_content'], axis=1, inplace=True)\n",
    "\n",
    "        # Rename columns\n",
    "        wiki_df = wiki_df.rename(columns={'raw_passage_content': 'contents', 'joint_id': 'id'})\n",
    "        \n",
    "        # Write to JSON file\n",
    "        json_filename = filename.split(\".\")[0] + \".json\"\n",
    "        wiki_df.to_json(output_dir+\"/\"+json_filename, orient='records')\n",
    "\n",
    "        print(f\"Processed {filename} and saved as {json_filename}\")\n",
    "\n",
    "    print(\"Conversion completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.parquet and saved as 0.json\n",
      "Processed 1.parquet and saved as 1.json\n",
      "Processed 2.parquet and saved as 2.json\n",
      "Processed 3.parquet and saved as 3.json\n",
      "Processed 4.parquet and saved as 4.json\n",
      "Processed 5.parquet and saved as 5.json\n",
      "Processed 6.parquet and saved as 6.json\n",
      "Processed 7.parquet and saved as 7.json\n",
      "Processed 8.parquet and saved as 8.json\n",
      "Processed 9.parquet and saved as 9.json\n",
      "Conversion completed\n"
     ]
    }
   ],
   "source": [
    "convert_json_to_pyserini_format_raw_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python faiss_1.7.4",
   "language": "python",
   "name": "faiss_1.7.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
